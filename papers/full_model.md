# LLM Architecture Papers

## GPT-2
[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## GPT-3
[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

## GPT-4
[GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774)

## InstructGPT
[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)

## LLaMA
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)

## LLaMA 2
[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)

## LLaMA 3
[The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)

## DeepSeek
[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/pdf/2401.02954)

## DeepSeek-V2
[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)

## DeepSeek-V3
[DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)

## DeepSeek-R1
[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)

## OLMo
[OLMo: Accelerating the Science of Language Models](https://arxiv.org/pdf/2402.00838)

## OLMo 2
[OLMo 2: Iterative Refinement Teaches Language Models to Respond](https://arxiv.org/pdf/2501.00656)

## OLMoE
[OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/pdf/2409.02060)

## Kimi 1.5
[Kimi 1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/pdf/2501.12599)

## Gemini 1.5
[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530)

## Gemini 2.5
[Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2507.06261)
