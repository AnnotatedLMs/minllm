# DeepSeek-V3 Configuration
# Based on DeepSeek-V3 architecture with MLA and MoE

model:
  architecture: deepseek3

  # Weight initialization (DeepSeek uses PyTorch defaults)
  weight_init:
    strategy: pytorch_default

  # Token embeddings
  token_embedding:
    vocab_size: 128000  # DeepSeek uses similar vocab to Llama
    embedding_dim: 7168
    embedding_dropout: 0.0
    init_std: 0.02  # Not used with pytorch_default strategy

  # Transformer backbone
  transformer_backbone:
    vocab_size: 128000
    hidden_dim: 7168
    n_layers: 61  # DeepSeek-V3 is very deep
    block_size: 4096
    dropout: 0.0
    bias: false

    # RMSNorm normalization
    normalization:
      norm_type: rmsnorm
      norm_eps: 0.00001  # 1e-5
      bias: false

    # Multi-head Latent Attention (MLA)
    attention:
      attention_type: multi_head_latent  # DeepSeek uses MLA
      num_heads: 128
      dropout: 0.0
      bias: false
      max_seq_length: 4096
      is_causal: true
      use_flash_attention: false  # MLA not compatible with Flash Attention
      # MLA specific parameters
      head_dim: 128
      kv_compression_dim: 512  # Compress KV to save memory
      query_compression_dim: 1536  # Compress Q for efficiency
      rope_dim: 64  # Separate RoPE dimension for MLA

    # RoPE position encoding (DeepSeek uses decoupled RoPE)
    rope:
      theta: 10000.0
      dim: 64  # Smaller RoPE dimension for MLA

    # Mixture of Experts (MoE)
    moe:
      num_experts: 16
      num_experts_per_token: 2  # Top-2 routing
      expert_type: ffn
      expert_intermediate_dim: 28672  # 4 * hidden_dim
      shared_expert_ratio: 0.1
      load_balancing_weight: 0.01
      router_type: topk
      moe_type: standard
      bias_update_speed: 0.001
      gate_noise_scale: 0.01

  # Output head
  output_head:
    tie_word_embeddings: true
    lm_head_bias: false

  # Multi-token prediction
  mtp:
    n_predict: 3  # Predict 3 future tokens
    prediction_depth: 1
    dropout_rate: 0.1

# Training configuration
training:
  # Batch settings
  batch_size: 2
  gradient_accumulation_steps: 64  # Effective batch size = 128
  max_iters: 50000
  log_interval: 10
  seed: 1337

  # Learning rate schedule
  lr_warmup_iters: 1000
  lr_decay_iters: 50000
  min_lr: 0.00001  # 1e-5

  # Evaluation
  eval_interval: 500
  eval_iters: 50

  # MoE-specific training
  moe_training:
    aux_loss_weight: 0.001    # Not used for aux_loss_free, but required by config
    capacity_factor: 1.25     # Token capacity per expert
    drop_tokens: true         # Drop tokens when experts full
    z_loss_weight: 0.001      # Router z-loss for stability

  # Multi-token prediction training
  mtp_training:
    mtp_loss_weight: 0.5  # Weight for auxiliary MTP loss

# Optimizer configuration
optimizer:
  optimizer_type: adamw
  learning_rate: 0.00006  # Lower LR for MoE models (6e-5)
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  eps: 0.00000001  # 1e-8
  parameter_grouping: dimension

# Learning rate schedule
lr_schedule:
  schedule_type: cosine
  warmup_iters: 1000
  lr_decay_iters: 50000
  min_lr: 0.00001  # 1e-5

# Checkpointing
checkpointing:
  save_dir: ./checkpoints/deepseek3
  save_interval: 2500
  save_best: true
  keep_last_n: 3

# Data configuration
data:
  dataset: redpajama
  data_dir: ./data/redpajama
  train_split: 0.99
  val_split: 0.01
  num_workers: 8
  pin_memory: true

# System configuration
system:
  device: cuda
  torch_dtype: bfloat16
  compile: true
  compile_mode: default
  distributed: true  # MoE models require distributed training
  backend: nccl
  find_unused_parameters: false

# Logging configuration
logging:
  enabled: true
  project: minllm-deepseek
  run_name: deepseek3-moe-mla
  log_model: false
