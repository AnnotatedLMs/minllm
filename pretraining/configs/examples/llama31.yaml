# Llama 3.1 8B Configuration
# Based on Meta's Llama 3.1 architecture

model:
  architecture: llama3.1

  # Weight initialization (Llama uses PyTorch defaults)
  weight_init:
    strategy: pytorch_default

  # Token embeddings
  token_embedding:
    vocab_size: 128256  # Llama 3.1 uses a larger vocabulary
    embedding_dim: 4096
    embedding_dropout: 0.0
    init_std: 0.02  # Not used with pytorch_default strategy

  # Transformer backbone
  transformer_backbone:
    vocab_size: 128256
    hidden_dim: 4096
    n_layers: 32
    block_size: 8192  # Llama 3.1 supports up to 128K context
    dropout: 0.0
    bias: false  # Llama uses no bias in any layers

    # RMSNorm normalization
    normalization:
      norm_type: rmsnorm
      norm_eps: 0.00001  # 1e-5
      bias: false

    # Grouped Query Attention (GQA)
    attention:
      attention_type: grouped_query  # Llama 3.1 uses GQA
      num_heads: 32
      num_kv_heads: 8  # GQA with 4:1 ratio
      dropout: 0.0
      bias: false
      max_seq_length: 8192
      is_causal: true
      use_flash_attention: true  # Llama can use Flash Attention with GQA

    # RoPE position encoding
    rope:
      theta: 500000.0
      scaling:  # For extended context
        scale_factor: 8.0
        low_freq_factor: 1.0
        high_freq_factor: 4.0
        original_context_len: 8192

    # SwiGLU feed-forward network
    ffn:
      ffn_type: swiglu
      intermediate_dim: 14336  # Adjusted to multiple of 256
      activation: silu
      ffn_dim_multiplier: 1.3  # Llama 3 uses this multiplier
      multiple_of: 256
      dropout: 0.0
      bias: false

  # Output head
  output_head:
    tie_word_embeddings: true  # Llama ties embeddings
    lm_head_bias: false

# Training configuration
training:
  # Batch settings
  batch_size: 4
  gradient_accumulation_steps: 32  # Effective batch size = 128
  max_iters: 100000
  log_interval: 10
  seed: 1337

  # Learning rate schedule
  lr_warmup_iters: 1000
  lr_decay_iters: 100000
  min_lr: 0.00001  # 1e-5  # 10% of max LR

  # Evaluation
  eval_interval: 1000
  eval_iters: 100

# Optimizer configuration
optimizer:
  optimizer_type: adamw
  learning_rate: 0.0001  # Max learning rate (1e-4)
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  eps: 0.00000001  # 1e-8
  parameter_grouping: dimension

# Learning rate schedule
lr_schedule:
  schedule_type: cosine
  warmup_iters: 1000
  lr_decay_iters: 100000
  min_lr: 0.00001  # 1e-5

# Checkpointing
checkpointing:
  save_dir: ./checkpoints/llama31
  save_interval: 5000
  save_best: true  # Save best model based on validation loss
  keep_last_n: 3

# Data configuration
data:
  dataset: redpajama
  data_dir: ./data/redpajama
  train_split: 0.99
  val_split: 0.01
  num_workers: 8
  pin_memory: true

# System configuration
system:
  device: cuda
  torch_dtype: bfloat16
  compile: true  # Use torch.compile
  compile_mode: default
  distributed: true  # Llama models typically need distributed training
  backend: nccl
  find_unused_parameters: false

# Logging configuration
logging:
  enabled: true
  project: minllm-llama
  run_name: llama31-8b
  log_model: false
