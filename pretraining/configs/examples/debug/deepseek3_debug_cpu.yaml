# DeepSeek-V3 Debug Configuration (Tiny model for testing)

model:
  vocab_size: 50257  # GPT-2 vocab for FineWeb

  # Token embeddings
  token_embedding:
    embedding_dim: 64  # Tiny dim
    init_std: 0.02

  # Transformer backbone
  transformer:
    hidden_dim: 64
    n_layers: 2  # Just 2 layers
    block_size: 128
    dropout: 0.0
    bias: false

    # RMSNorm normalization
    normalization:
      norm_eps: 0.00001  # 1e-5

    # Multi-head Latent Attention (MLA)
    attention:
      num_heads: 4
      bias: false
      max_seq_length: 128
      is_causal: true
      use_flash_attention: false
      # MLA specific parameters
      head_dim: 16
      kv_compression_dim: 32  # Compress KV to save memory
      query_compression_dim: 48  # Compress Q for efficiency
      rope_dim: 8  # Separate RoPE dimension for MLA

    # RoPE position encoding (DeepSeek uses decoupled RoPE)
    rope:
      theta: 10000.0
      dim: 8  # Smaller RoPE dimension for MLA

    # Mixture of Experts (MoE)
    moe:
      num_experts: 4
      num_experts_per_token: 2  # Top-2 routing
      intermediate_dim: 256  # FFN hidden dimension per expert
      activation: silu
      shared_expert_ratio: 0.1
      n_shared_experts: 2
      bias_update_speed: 0.001
      aux_loss_alpha: 0.001  # Extremely small factor for auxiliary loss (DeepSeek-V3 paper)
      gate_noise_scale: 0.01

  # Output head
  output_head:
    tie_word_embeddings: false
    lm_head_bias: false

  # Multi-token prediction
  mtp:
    n_predict: 2  # Predict 2 future tokens
    prediction_depth: 1
    dropout: 0.1

# Training configuration
training:
  # Top-level training loop fields
  token_budget: 2560  # 10 steps × 2 batch × 128 seq_len
  max_iters: 10  # Safety limit for debugging
  log_interval: 1
  seed: 1337
  gradient_accumulation_steps: 1
  precision: fp32

  # Data configuration
  data:
    dataset: fineweb10B
    data_dir: ./pretraining/data/sample/fineweb10B
    num_workers: 1
    pin_memory: false

  # Batch configuration
  batch:
    batch_size: 2
    sequence_length: 128

  # Optimizer configuration
  optimizer:
    optimizer_type: adamw
    learning_rate: 0.00006
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    eps: 0.00000001
    parameter_grouping: dimension

  # Learning rate schedule
  lr_schedule:
    schedule_type: cosine
    warmup_iters: 2
    lr_decay_iters: 10
    min_lr: 0.00001

  # Loss configuration
  loss:
    cross_entropy_weight: 1.0
    moe_aux_loss_weight: 0.01
    mtp_loss_weight: 0.3

  # Evaluation configuration
  evaluation:
    eval_interval: 5
    eval_iters: 2
    num_eval_batches: 50

  # Checkpointing configuration
  checkpoint:
    save_dir: ./checkpoints/debug/deepseek3
    save_interval: 10
    save_best: false
    keep_last_n: 1

  # Torch compilation configuration
  torch_compilation:
    compile: false

  # Execution configuration
  execution:
    strategy: single
    single:
      device: cpu  # Use CPU for debug

  # Weights & Biases logging
  wandb_logging:
    enabled: true
    project: minllm-debug
    entity: null  # Set your W&B entity/username
    group: null
    name: debug-deepseek3
    tags: ["debug", "deepseek3", "moe", "test"]
    log_artifacts: false
    rank_zero_only: true
    log_interval: 1

  # MoE-specific training
  moe_training:
    capacity_factor: 1.25     # Token capacity per expert
    drop_tokens: true         # Drop tokens when experts full
