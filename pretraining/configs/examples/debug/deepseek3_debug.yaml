# DeepSeek-V3 Debug Configuration (Tiny model for testing)

model:
  architecture: deepseek3

  # Weight initialization
  weight_init:
    strategy: pytorch_default

  # Token embeddings
  token_embedding:
    vocab_size: 1000  # Tiny vocab
    embedding_dim: 64  # Tiny dim
    embedding_dropout: 0.0
    init_std: 0.02

  # Transformer backbone
  transformer_backbone:
    vocab_size: 1000
    hidden_dim: 64
    n_layers: 2  # Just 2 layers
    block_size: 128
    dropout: 0.0
    bias: false

    # RMSNorm normalization
    normalization:
      norm_type: rmsnorm
      norm_eps: 0.00001  # 1e-5
      bias: false

    # Multi-head Latent Attention (MLA)
    attention:
      attention_type: multi_head_latent
      num_heads: 4
      dropout: 0.0
      bias: false
      max_seq_length: 128
      is_causal: true
      # MLA specific parameters
      head_dim: 16
      kv_compression_dim: 32  # Compress KV to save memory
      query_compression_dim: 48  # Compress Q for efficiency
      rope_dim: 8  # Separate RoPE dimension for MLA

    # RoPE position encoding (DeepSeek uses decoupled RoPE)
    rope:
      theta: 10000.0
      dim: 8  # Smaller RoPE dimension for MLA

    # Mixture of Experts (MoE)
    moe:
      num_experts: 4  # Just 4 experts
      num_experts_per_token: 2  # Top-2 routing
      expert_type: ffn
      expert_intermediate_dim: 256  # 4 * hidden_dim
      shared_expert_ratio: 0.1
      load_balancing_weight: 0.01
      router_type: topk
      moe_type: aux_loss_free  # DeepSeek uses aux-loss-free
      bias_update_speed: 0.001
      gate_noise_scale: 0.01

  # Output head
  output_head:
    tie_word_embeddings: true
    lm_head_bias: false

  # Multi-token prediction
  mtp:
    n_predict: 2  # Predict 2 future tokens
    prediction_depth: 1
    dropout_rate: 0.1

# Training configuration
training:
  # Batch settings
  batch_size: 2
  gradient_accumulation_steps: 1
  max_iters: 10
  log_interval: 1
  seed: 1337

  # Learning rate schedule
  lr_warmup_iters: 2
  lr_decay_iters: 10
  min_lr: 0.00001

  # Evaluation
  eval_interval: 5
  eval_iters: 2

  # MoE-specific training
  moe_training:
    aux_loss_weight: 0.001    # Not used for aux_loss_free, but required by config
    capacity_factor: 1.25     # Token capacity per expert
    drop_tokens: true         # Drop tokens when experts full
    z_loss_weight: 0.001      # Router z-loss for stability

  # Multi-token prediction training
  mtp_training:
    mtp_loss_weight: 0.5  # Weight for auxiliary MTP loss

# Optimizer configuration
optimizer:
  optimizer_type: adamw
  learning_rate: 0.00006
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  eps: 0.00000001
  parameter_grouping: dimension

# Learning rate schedule
lr_schedule:
  schedule_type: cosine
  warmup_iters: 2
  lr_decay_iters: 10
  min_lr: 0.00001

# Checkpointing
checkpointing:
  save_dir: ./checkpoints/debug/deepseek3
  save_interval: 10
  save_best: false
  keep_last_n: 1

# Data configuration
data:
  dataset: debug
  data_dir: ./data/debug
  train_split: 0.9
  val_split: 0.1
  num_workers: 1
  pin_memory: false

# System configuration
system:
  device: cpu  # Use CPU for debug
  torch_dtype: float32
  compile: false
  distributed: false  # MoE models typically need distributed, but not for debug
  backend: nccl
  find_unused_parameters: false

# Logging configuration
logging:
  enabled: false
  project: debug
  run_name: debug-deepseek3
  log_model: false
