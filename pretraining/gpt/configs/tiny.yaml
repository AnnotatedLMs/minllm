# MinLLM Tiny Configuration
# Very small model for debugging and testing

# Model Architecture (tiny)
n_layer: 2
n_head: 2
n_embd: 128
n_head_dim: 64  # n_embd // n_head
vocab_size: 256  # Small vocab for testing
block_size: 64   # Short sequences
dropout: 0.1
bias: true

# Training (fast iteration)
batch_size: 4
gradient_accumulation_steps: 1
max_iters: 100
eval_interval: 10
eval_iters: 5
log_interval: 1
always_save_checkpoint: false
init_from: 'scratch'

# Optimizer
learning_rate: 1e-3
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# Learning Rate Schedule
decay_lr: false  # No decay for debugging
warmup_iters: 10
lr_decay_iters: 100
min_lr: 1e-4

# Data
dataset: 'tiny_shakespeare'  # Assuming a small debug dataset
data_dir: 'data'

# System
device: 'cuda'
dtype: 'float32'  # Full precision for debugging
compile: true
backend: 'nccl'

# Logging
out_dir: 'out-tiny'
wandb_log: false
wandb_project: 'minllm'
wandb_run_name: 'tiny-debug'

# Reproducibility
seed: 42
